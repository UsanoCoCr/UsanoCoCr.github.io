

  <!DOCTYPE html>
  <html lang="en">
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta name="description" content=一名在元培学院通用人工智能方向上学的本科生，敬上！ >
  <meta name="keywords" content=hexo,theme,notes >

  <head>
    <title>
      深度强化学习的一般算法 [ UsanoCoCr&#39;s Blog ]
    </title>
  <meta name="generator" content="Hexo 6.3.0"></head>

  <body>

    <link rel="stylesheet" href="/css/header.css">
<div class="header">
  <div class="logo">
    <span class="pull-left">
      <a id="site-name" href="/">
        UsanoCoCr&#39;s blog
      </a>
    </span>
  </div>
  <ul class="nav-list">
    
      <li>
        <a href="/">
          首页
        </a>
      </li>
      
  </ul>
</div>

      <!--<link rel="stylesheet" href="/css/top-header.css">
<div id="top-bar" class="fixed">

  <a class="goto-top" href="#"></a>
  <ul class="bar-list bar-list-1">
    
      <li>
        <p>
          <a href="/">
            <text class="bar-text bar-p1">
              首页
            </text>
            <text class="bar-text bar-p2"></text>
          </a>
          <text class="bar-p3">/</text>
        </p>
      </li>
      
  </ul>
</div>-->

        <div id="content-outer">
          <div class="content-inner">
            <link rel="stylesheet" href="/css/post.css">
<div class="posts">
  <a href="/index.html"><i class="fa fa-home
replay-btn" aria-hidden="true"></i></a>
  <div class="post-title">
    <p>
      深度强化学习的一般算法
    </p>
    <hr>
  </div>
  <div class="post-content">
    <h1 id="深度强化学习"><a href="#深度强化学习" class="headerlink" title="深度强化学习"></a>深度强化学习</h1><h2 id="应用ε-贪心的蒙特卡洛算法"><a href="#应用ε-贪心的蒙特卡洛算法" class="headerlink" title="应用ε-贪心的蒙特卡洛算法"></a>应用ε-贪心的蒙特卡洛算法</h2><p>在应用ε-贪心的蒙特卡洛算法中，我们在策略π的基础上，在每次迭代中找到所经过的状态-动作对，然后更新Q值，最后再根据更新后的Q值来更新策略π。<br>注意：使用ε-贪心，意味着在更新迭代时，有一部分动作的概率是ε&#x2F;动作总数量，选择最优策略的概率是1-ε。</p>
<h2 id="TD算法"><a href="#TD算法" class="headerlink" title="TD算法"></a>TD算法</h2><p>TD算法和蒙特卡洛算法有一些不同：</p>
<ul>
<li>TD在有或者没有结果的情况下均可以学习(如不必等一盘象棋结束)；MC则必须等待结束得到结果。TD胜！</li>
<li>TD在更新状态价值时使用的是TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，是当前状态价值的有偏估计；而MC则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计，MC胜！</li>
<li>虽然TD得到的价值是有偏估计，但是其方差却比MC得到的方差要低，且对初始值敏感，通常比蒙特卡罗法更加高效。TD胜！</li>
</ul>
<p><strong>在强化学习的应用中，有两种时序差分算法，分别是SARSA算法和Q-learning算法，它们都可以被运用到实践中。</strong></p>
<h2 id="SARSA算法"><a href="#SARSA算法" class="headerlink" title="SARSA算法"></a>SARSA算法</h2><ul>
<li>SARSA算法是一种基于值的强化学习算法，它的全称是State-Action-Reward-State-Action，即状态-动作-奖励-状态-动作。</li>
<li>SARSA算法的核心思想是：在每一步更新Q值时，都<strong>使用ε-贪心最优的动作来更新Q值</strong>，即$Q(s,a)←Q(s,a)+α(r+γQ(s’,a’)-Q(s,a))$</li>
<li>SARSA算法的伪代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Initialize Q(s,a),∀s∈S,a∈A(s),Q(terminal-state,·)=0</span><br><span class="line">Repeat (for each episode):</span><br><span class="line">    Initialize s</span><br><span class="line">    Choose a from s using policy derived from Q (e.g., ε-greedy)</span><br><span class="line">    Repeat (for each step of episode):</span><br><span class="line">        Take action a, observe r, s&#x27;</span><br><span class="line">        Choose a&#x27; from s&#x27; using policy derived from Q (e.g., ε-greedy)</span><br><span class="line">        Q(s,a)←Q(s,a)+α(r+γQ(s&#x27;,a&#x27;)-Q(s,a))</span><br><span class="line">        s←s&#x27;; a←a&#x27;;</span><br><span class="line">    until s is terminal</span><br></pre></td></tr></table></figure>
SARSA通过策略给定最优的第二步来估计当前最优的第一步，如果第二步的效果很好，那么Q值就会被更新为更大的值，反之则会被更新为更小的值</li>
</ul>
<h2 id="Q-learning算法"><a href="#Q-learning算法" class="headerlink" title="Q-learning算法"></a>Q-learning算法</h2><ul>
<li>Q-learning算法是一种基于值的强化学习算法，它的全称是Quality-Learning，即质量学习。</li>
<li>Q-learning算法的核心思想是：在每一步更新Q值时，都<strong>使用下一状态下的最大动作来更新Q值</strong>，即:$Q(s,a)←Q(s,a)+α(r+γ*argmax_aQ(s’,a)-Q(s,a))$</li>
<li>Q-learning算法的伪代码如下：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Initialize Q(s,a),∀s∈S,a∈A(s),Q(terminal-state,·)=0</span><br><span class="line">Repeat (for each episode):</span><br><span class="line">    Initialize s</span><br><span class="line">    Repeat (for each step of episode):</span><br><span class="line">        Choose a from s using policy derived from Q (e.g., ε-greedy)</span><br><span class="line">        Take action a, observe r, s&#x27;</span><br><span class="line">        Q(s,a)←Q(s,a)+α(r+γ*argmax_aQ(s&#x27;,a)-Q(s,a))</span><br><span class="line">        s←s&#x27;</span><br><span class="line">    until s is terminal</span><br></pre></td></tr></table></figure>
<strong>Q-learning通过策略给定最优（不进行贪心了！）的第二步来估计当前最优的第一步</strong>，如果第二步的效果很好，那么Q值就会被更新为更大的值，反之则会被更新为更小的值</li>
</ul>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><h3 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h3><p>对于片段性问题，定义策略π的性能为$J(θ)&#x3D;v_{πθ}(S_0)$<br>在这里，θ是策略π的参数，v是状态价值函数，$S_0$是初始状态。θ的参数往往通过神经网络进行设定。<br>则策略梯度定理告诉我们，策略π的梯度为：<br>$∇_θJ(θ)&#x3D;E_π[∑_a∇_θπ(a_t|s_t)Q_π(s_t,a_t)]$<br>其中，$Q_π(s_t,a_t)$是在状态$s_t$下采取动作$a_t$的价值，$∇_θπ(a_t|s_t)$是在状态$s_t$下采取动作$a_t$的概率的梯度。</p>
<h3 id="蒙特卡洛策略梯度算法"><a href="#蒙特卡洛策略梯度算法" class="headerlink" title="蒙特卡洛策略梯度算法"></a>蒙特卡洛策略梯度算法</h3><p>将策略梯度定理中的公式转化为基于采样的、期望相等的近似公式<br>则状态$S_t$的价值变化率为：$G_t&#x2F;π(a_t|s_t,θ)*∇_θπ(a_t|s_t,θ)$<br>状态$S_t$的价值变化率乘以衰减率γ，则可以生成一个序列，得到$S_0$的价值变化率：<br>$∇J(θ)&#x3D;E_π[∑_tγ^tG_t&#x2F;π(a_t|s_t,θ)*∇_θπ(a_t|s_t,θ)]$</p>
<p>下面给出蒙特卡洛策略梯度算法的伪代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Initialize θ</span><br><span class="line">Repeat (for each episode):</span><br><span class="line">    Generate an episode S_0,A_0,R_1,...,S_T-1,A_T-1,R_T following π(·|·,θ)</span><br><span class="line">    For each step of episode t=0,1,...,T-1:</span><br><span class="line">        G←∑_(k=t+1)^T*γ^(k-t-1)*R_k</span><br><span class="line">        θ←θ+αγ^tG∇_θlnπ(A_t|S_t,θ)</span><br></pre></td></tr></table></figure>

<h3 id="蒙特卡洛策略梯度算法-with-baseline"><a href="#蒙特卡洛策略梯度算法-with-baseline" class="headerlink" title="蒙特卡洛策略梯度算法 with baseline"></a>蒙特卡洛策略梯度算法 with baseline</h3><p>在蒙特卡洛策略梯度算法中，我们可以<strong>引入一个baseline，即$b(S_t)$，来减少方差，使得算法更加稳定</strong>。<br>在策略梯度定理中减去baseline之后，原定理变为：<br>$∇_θJ(θ)&#x3D;E_π[∑_a∇_θπ(a_t|s_t)(Q_π(s_t,a_t)-b(s_t))]$<br>一般令$b(S_t)&#x3D;v(S_t,w)$，其中另一个神经网络W的参数为w，v是状态价值函数，$S_t$是状态。<br>则蒙特卡洛策略梯度算法 with baseline的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Initialize θ,w</span><br><span class="line">Repeat (for each episode):</span><br><span class="line">    Generate an episode S_0,A_0,R_1,...,S_T-1,A_T-1,R_T following π(·|·,θ)</span><br><span class="line">    For each step of episode t=0,1,...,T-1:</span><br><span class="line">        G←∑_(k=t+1)^T*γ^(k-t-1)*R_k</span><br><span class="line">        δ←G-v(S_t,w)</span><br><span class="line">        θ←θ+αγ^t*δ*∇_θlnπ(A_t|S_t,θ)</span><br><span class="line">        w←w+αγ^t*δ*∇_wv(S_t,w)</span><br></pre></td></tr></table></figure>

<h3 id="Actor-Critic算法"><a href="#Actor-Critic算法" class="headerlink" title="Actor-Critic算法"></a>Actor-Critic算法</h3><p>将蒙特卡洛策略梯度算法 with baseline中的蒙特卡洛算法替换为时序差分算法，即将全部return替换为单步return，可以获得单步Actor-Critic算法。<br>其中：</p>
<ul>
<li>Actor是按照策略梯度算法的策略$π(A_t|S_t,θ)$决定下一步的行动</li>
<li>Critic是利用状态价值函数$v(S_t,w)$来评价策略的好坏</li>
</ul>
<p>Actor-Critic算法的伪代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Initialize θ,w,I=1</span><br><span class="line">Repeat (for each episode):</span><br><span class="line">    Generate an episode S_0,A_0,R_1,...,S_T-1,A_T-1,R_T following π(·|·,θ)</span><br><span class="line">    while S_t is not terminal:</span><br><span class="line">        δ←R_t+γv(S_t+1,w)-v(S_t,w)</span><br><span class="line">        θ←θ+αI*δ*∇_θlnπ(A_t|S_t,θ)</span><br><span class="line">        w←w+αI*δ*∇_wv(S_t,w)</span><br><span class="line">        I←I*γ</span><br><span class="line">        S_t←S_t+1</span><br></pre></td></tr></table></figure>
  </div>
  
</div>
<script type="text/javascript" src="../source/libs/codeBlock/codeBlockFuction.js"></script>
<!-- 代码语言 -->
<script type="text/javascript" src="../source/libs/codeBlock/codeLang.js"></script>
<!-- 代码块复制 -->
<script type="text/javascript" src="../source/libs/codeBlock/codeCopy.js"></script>
<script type="text/javascript" src="../source/libs/codeBlock/clipboard.min.js"></script>
<!-- 代码块收缩 -->
<script type="text/javascript" src="../source/libs/codeBlock/codeShrink.js"></script> 
<!-- 代码块折行 -->
<style type="text/css">code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }</style>
          </div>
        </div>

        <link rel="stylesheet" href="/css/footer.css">
<div class="bottom-outer">
  <div class="copyright">©2021 - 2021 By Tanger</div>
  <div class="framework-info">
    <span>Power by</span>
    <a class="a1" target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>
    <span class="footer-separator">|</span>
    <span>Theme by</span>
    <a class="a2" target="_blank" rel="noopener" href="https://github.com/redhat123456/hexo-theme-MiHoYo">MiHoYo</a>
  </div>
</div>

          
            <!-- scripts list from theme config.yml -->
            
              <script src="/js/MiHoYo.js"></script>
              
                

  </body>

  </html>